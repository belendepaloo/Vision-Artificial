{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Método de imágenes panorámicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@115.056] global loadsave.cpp:241 findDecoder imread_('Castañeda_Depalo_tp1_pano/img/udesa_0.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@115.056] global loadsave.cpp:241 findDecoder imread_('Castañeda_Depalo_tp1_pano/img/udesa_1.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@115.056] global loadsave.cpp:241 findDecoder imread_('Castañeda_Depalo_tp1_pano/img/udesa_2.jpg'): can't open/read file: check file path/integrity\n"
     ]
    }
   ],
   "source": [
    "#Importamos las imagenes \n",
    "img0 = cv2.imread(\"Castañeda_Depalo_tp1_pano/img/udesa_0.jpg\")\n",
    "img1 = cv2.imread(\"Castañeda_Depalo_tp1_pano/img/udesa_1.jpg\")\n",
    "img2 = cv2.imread(\"Castañeda_Depalo_tp1_pano/img/udesa_2.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Detección y descripción de características visuales (features)\n",
    "En la mayoría de las tareas de visión por computadora, el primer paso para unir un\n",
    "panorama es detectar y extraer características visuales distintivas.\n",
    "Pueden seleccionar el tipo de características visuales que prefieran para detectar\n",
    "y extraer de las imágenes. Pueden utilizar las funciones de OpenCV para esto, tal\n",
    "como fue visto en las clases tutoriales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.10.0) /private/var/folders/c_/qfmhj66j0tn016nkx_th4hxm0000gp/T/abs_385j4q2b1p/croot/opencv-suite_1744808142631/work/modules/imgproc/src/color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31merror\u001b[39m                                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#Metodo de Harris\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m gray = \u001b[43mcv2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcvtColor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCOLOR_BGR2GRAY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\u001b[32m      5\u001b[39m plt.title(\u001b[33m\"\u001b[39m\u001b[33moriginal\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31merror\u001b[39m: OpenCV(4.10.0) /private/var/folders/c_/qfmhj66j0tn016nkx_th4hxm0000gp/T/abs_385j4q2b1p/croot/opencv-suite_1744808142631/work/modules/imgproc/src/color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"
     ]
    }
   ],
   "source": [
    "#Metodo de Harris\n",
    "\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "plt.title(\"original\")\n",
    "plt.imshow(rgb)\n",
    "plt.show()\n",
    "\n",
    "gray = np.float32(gray)\n",
    "R = cv2.cornerHarris(gray, 2, 3, 0.04)\n",
    "R = cv2.normalize(R, None, 0, 1, cv2.NORM_MINMAX)\n",
    "\n",
    "plt.imshow(R, cmap='gray')\n",
    "plt.title(\"response\")\n",
    "plt.show()\n",
    "\n",
    "# threshold\n",
    "thresh = 1.01 * np.median(R)\n",
    "\n",
    "x, y = np.where(R > thresh)\n",
    "plt.imshow(R, cmap='gray')\n",
    "plt.scatter(y, x, s=1, color='lightgreen')\n",
    "plt.title(\"threshold\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Supresión de No Máxima Adaptativa (Adaptive Non-Maximal Suppression, ANMS)\n",
    "Las características visuales podrían estar condensadas en una porción pequeña\n",
    "de la imagen. Para poder encontrar transformaciones de calidad nos interesa que\n",
    "las características visuales estén “bien” distribuidas en la imagen.\n",
    "Este paso tendrá entonces dos objetivos:\n",
    "1. Distribuir mejor las coordenadas de los puntos clave.\n",
    "2. Acotar la cantidad máxima de características visuales que se utilizarán de\n",
    "la imágen quedándonos con las N mejores que maximicen, además, la\n",
    "distancia entre ellas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Asociación de características (Matching)\n",
    "\n",
    "En este paso queremos encontrar **buenas correspondencias entre los descriptores** de las diferentes imágenes.\n",
    "\n",
    "La asociación de características se realiza entre pares de imágenes. Para cada descriptor de la **imagen 1**, buscaremos **los dos vecinos más cercanos** entre los descriptores de la **imagen 2**.\n",
    "\n",
    "Para esto, pueden utilizar:\n",
    "\n",
    "- `cv2.BFMatcher`\n",
    "- `cv2.FlannBasedMatcher`\n",
    "\n",
    "Con base en esto, deberán realizar la **asociación de correspondencias entre las tres imágenes de la panorámica**.  \n",
    "Se recomienda definir una **imagen como “ancla”**, a la cual se le buscará relacionar y “juntar” las dos imágenes restantes.\n",
    "\n",
    "Deberán aplicar las siguientes técnicas:\n",
    "\n",
    "- **Verificación cruzada (cross-check)**:  \n",
    "  Una asociación es considerada válida **solo si se obtiene el mismo par de asociaciones en ambas direcciones**.  \n",
    "  Es decir, cada *keypoint* debe ser la mejor asociación del otro.  \n",
    "  ➤ Verificar la documentación de OpenCV sobre cómo implementarlo.\n",
    "\n",
    "- **Comprobación de Lowe (Lowe's ratio test)**:  \n",
    "  Quedarse solamente con aquellas correspondencias en las que el **segundo vecino más cercano esté al menos a una distancia mayor** que un determinado *threshold*.  \n",
    "  Recordar:\n",
    "  - Las distancias dependen del tipo de descriptor:\n",
    "    - **BRIEF / ORB**: usan **distancia de Hamming**\n",
    "    - **SIFT**: usa **norma 2 (Euclídea)**\n",
    "\n",
    "\n",
    "**Sugerencia**:  \n",
    "Pueden comparar el resultado de aplicar cada política por separado, justificar su selección o incluso **implementar una combinación de ambas.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Estimación de la homografía “manualmente”\n",
    "\n",
    "En esta etapa se buscará **estimar una homografía entre la imagen central (o “ancla”) y alguna otra**, determinando **manualmente 4 pares de correspondencias**.  \n",
    "Pueden usar un editor de imágenes para obtener las coordenadas de los puntos de interés.\n",
    "\n",
    "Pasos a seguir:\n",
    "\n",
    "1. Seleccionar **4 pares de puntos correspondientes** entre la imagen ancla y otra imagen.\n",
    "2. Aplicar el algoritmo **DLT (Direct Linear Transformation)** para hallar la matriz de homografía utilizando esos puntos.\n",
    "3. Repetir el proceso para ambos pares de imágenes respecto a la imagen ancla.\n",
    "\n",
    "**Restricción**:  \n",
    "Deben **implementar el algoritmo DLT ustedes mismos**,  \n",
    "**sin utilizar la función de OpenCV para estimar homografías** (`cv2.findHomography` o similares).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5. RANSAC para eliminación de outliers y estimación de homografías\n",
    "\n",
    "En esta etapa se buscará nuevamente **estimar una homografía entre la imagen central (o “ancla”) y otra imagen**, pero esta vez de forma **algorítmica utilizando RANSAC**.\n",
    "\n",
    "- Implementar **su propia versión del algoritmo RANSAC**, sin utilizar funciones de OpenCV.\n",
    "- El objetivo es:\n",
    "  - **Eliminar outliers** en las correspondencias.\n",
    "  - **Conservar únicamente los inliers** para estimar una homografía más robusta.\n",
    "\n",
    "Pasos:\n",
    "\n",
    "1. Ejecutar su implementación de **RANSAC** sobre las correspondencias clave entre las imágenes.\n",
    "2. Una vez obtenido el **conjunto de inliers**, utilizar `cv2.findHomography` **solo con los inliers** para calcular la homografía final.\n",
    "   - **No deben usar el parámetro de RANSAC dentro de `cv2.findHomography`** (es decir, **¡debe estar desactivado!**).\n",
    "\n",
    "\n",
    "**Nota**: Esta etapa mejora la robustez del emparejamiento, ya que descarta puntos erróneos que podrían afectar la estimación de la homografía.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6. Cálculo de tamaño óptimo para imagen final\n",
    "Deberán calcular y ajustar las transformaciones de forma tal que no sobren\n",
    "bordes innecesarios en la imagen final. Tampoco deben quedar recortadas las\n",
    "imágenes transformadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7. Juntando y mezclando imágenes (Stitching and Blending)\n",
    "\n",
    "Para juntar las imágenes relacionadas, primero es necesario calcular el **tamaño final de la imagen panorámica**.  \n",
    "Es posible lograr esto inspeccionando las **coordenadas máximas y mínimas** que se obtienen al **transformar las imágenes mediante las homografías** \\( H \\).\n",
    "\n",
    "Si se van a pegar múltiples imágenes, es importante tener en cuenta:\n",
    "\n",
    "- Todos los **vértices** de las imágenes.\n",
    "- Dónde irán a parar al aplicar **cada homografía \\( H \\)**.\n",
    "- Las **traslaciones necesarias** que deben agregarse a las homografías para que el stitching final quede correctamente alineado.\n",
    "\n",
    "\n",
    "Para cada imagen se debe realizar el **warping hacia la imagen “ancla”**.  \n",
    "Usando las homografías encontradas, pueden utilizar las funciones de OpenCV:\n",
    "\n",
    "- `cv2.perspectiveTransform`\n",
    "- `cv2.warpPerspective`\n",
    "\n",
    "\n",
    "#### Blending (Mezcla de color)\n",
    "\n",
    "Finalmente, pueden aplicar una **corrección de color** realizando una **mezcla (blending)** de los canales de color de las imágenes unidas.\n",
    "\n",
    "- Se recomienda crear una **máscara de “degradé”** para cada imagen utilizando `cv2.distanceTransform`, la cual se llamará \\( W \\).\n",
    "\n",
    "- Para mezclar un canal de color, se aplica la correspondiente máscara \\( W \\) a cada imagen, y luego se unen todas utilizando la siguiente ecuación:\n",
    "\n",
    "$$\n",
    "I = \\frac{\\sum_i W_i \\cdot I_i}{\\sum_i W_i}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
